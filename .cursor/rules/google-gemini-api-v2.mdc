---
description: 
globs: 
alwaysApply: true
---
## Google Gemini API Notes

### Text Generation

The Gemini API can generate text output in response to various inputs, including text, images, video, and audio. This guide shows you how to generate text using text and image inputs. It also covers streaming, chat, and system instructions.

#### Text Input

The simplest way to generate text using the Gemini API is to provide the model with a single text-only input, as shown in this example:

```python
from google import genai

client = genai.Client(api_key="GEMINI_API_KEY")

response = client.models.generate_content(
    model="gemini-2.0-flash",
    contents=["How does AI work?"]
)
print(response.text)
```

#### Streaming Output

By default, the model returns a response after completing the entire text generation process. You can achieve faster interactions by using streaming to return instances of `GenerateContentResponse` as they're generated.

```python
from google import genai

client = genai.Client(api_key="GEMINI_API_KEY")

response = client.models.generate_content_stream(
    model="gemini-2.0-flash",
    contents=["Explain how AI works"]
)
for chunk in response:
    print(chunk.text, end="")
```

#### Multi-turn Conversations

The Gemini SDK lets you collect multiple rounds of questions and responses into a chat. The chat format enables users to step incrementally toward answers and to get help with multipart problems. This SDK implementation of chat provides an interface to keep track of conversation history, but behind the scenes it uses the same `generateContent` method to create the response.

The following code example shows a basic chat implementation:

```python
from google import genai

client = genai.Client(api_key="GEMINI_API_KEY")
chat = client.chats.create(model="gemini-2.0-flash")

response = chat.send_message("I have 2 dogs in my house.")
print(response.text)

response = chat.send_message("How many paws are in my house?")
print(response.text)

# Print chat history
for message in chat.get_history():
    print(f'role - {message.role}',end=": ")
    print(message.parts[0].text)
```

You can also use streaming with chat, as shown in the following example:

```python
from google import genai

client = genai.Client(api_key="GEMINI_API_KEY")
chat = client.chats.create(model="gemini-2.0-flash")

response = chat.send_message_stream("I have 2 dogs in my house.")
for chunk in response:
    print(chunk.text, end="")
print() # Add a newline after the first streamed response

response = chat.send_message_stream("How many paws are in my house?")
for chunk in response:
    print(chunk.text, end="")
print() # Add a newline after the second streamed response

# Print chat history
for message in chat.get_history():
    print(f'role - {message.role}', end=": ")
    print(message.parts[0].text)
```

#### System Instructions

System instructions let you steer the behavior of a model based on your specific use case. When you provide system instructions, you give the model additional context to help it understand the task and generate more customized responses. The model should adhere to the system instructions over the full interaction with the user, enabling you to specify product-level behavior separate from the prompts provided by end users.

You can set system instructions when you initialize your model:

```python
from google import genai
from google.genai import types

client = genai.Client(api_key="GEMINI_API_KEY")

response = client.models.generate_content(
    model="gemini-2.0-flash",
    config=types.GenerateContentConfig(
        system_instruction="You are a cat. Your name is Neko."),
    contents="Hello there"
)

print(response.text)
```

### Use Gemini Thinking

Gemini 2.5 Pro Experimental and Gemini 2.0 Flash Thinking Experimental are models that use an internal "thinking process" during response generation. This process contributes to their improved reasoning capabilities and allows them to solve complex tasks. This guide shows you how to use Gemini models with thinking capabilities.

*Try Gemini 2.5 Pro Experimental in Google AI Studio*

#### Use Thinking Models

Models with thinking capabilities are available in Google AI Studio and through the Gemini API. Note that the thinking process is visible within Google AI Studio but is not provided as part of the API output.

##### Send a Basic Request

```python
from google import genai

client = genai.Client(api_key="GEMINI_API_KEY")
prompt = "Explain the concept of Occam's Razor and provide a simple, everyday example."
response = client.models.generate_content(
    model="gemini-2.5-pro-exp-03-25",  # or gemini-2.0-flash-thinking-exp
    contents=prompt
)

print(response.text)
```

##### Multi-turn Thinking Conversations

To take the previous chat history into account, you can use multi-turn conversations.

With the SDKs, you can create a chat session to manage the state of the conversation. *Note: The example below uses `asyncio`.*

```python
import asyncio
from google import genai

async def run_async_chat():
    client = genai.Client(api_key='GEMINI_API_KEY')

    chat = client.aio.chats.create(
        model='gemini-2.5-pro-exp-03-25',  # or gemini-2.0-flash-thinking-exp
    )
    response = await chat.send_message('What is your name?')
    print(response.text)
    response = await chat.send_message('What did you just say before this?')
    print(response.text)

# To run the async function:
# asyncio.run(run_async_chat())
```